{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Luis Antonio Ortega Andrés     \n",
    "Antonio Coín Castro*\n",
    "\n",
    "# Métodos Avanzados en Aprendizaje Automático \n",
    "# Práctica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de Eliminación de Variables\n",
    "\n",
    "Este algoritmo se utiliza para hacer inferencia en redes. Supongamos que tenemos la factorización de una distribución conjunta \n",
    "\n",
    "$$ P(\\mathbf{X}) = P(X_1, X_2, \\dots, X_N) = \\prod_{i=1}^N P(X_i|Par(X_i))$$ \n",
    "\n",
    "y una evidenica $ \\mathbf{Z}=\\mathbf{z} $, donde $\\mathbf{Z} \\subset \\mathbf{X}$ es un subconjunto de las variables del problema y $\\mathbf{z}$ son sus valores observados. El objetivo es obtener la distribución de parte de las variables del problema, $\\mathbf{W} \\subset \\mathbf{X}$, dada la evidencia $\\mathbf{Z}=\\mathbf{z}$. Es decir, queremos obtener $P(\\mathbf{W}|\\mathbf{Z}=\\mathbf{z})$:\n",
    "\n",
    "$$ P(\\mathbf{W}|\\mathbf{Z}=\\mathbf{z}) = \\sum_{X \\setminus (W\\cup Z)} \\frac{P(\\mathbf{X}\\setminus \\mathbf{Z},\\mathbf{Z}=\\mathbf{z})}{P(\\mathbf{Z}=\\mathbf{z})} \\propto \\sum_{X \\setminus (W\\cup Z)} P(\\mathbf{X}\\setminus \\mathbf{Z},\\mathbf{Z}=\\mathbf{z}).$$\n",
    "\n",
    "Para ello debemos:\n",
    "\n",
    "* Reducir los factores que incluyan $\\mathbf{Z}$.\n",
    "* Eliminar el resto de variables no incluidas $\\mathbf{W}$.\n",
    "\n",
    "\n",
    "Algoritmo de eliminación de variables esquemático para un conjunto de factores $\\mathbf{\\Phi}=\\{\\Phi_1,\\dots,\\Phi_N\\}$:\n",
    "1.  Reducir todos los factores que contengan alguna variable de $\\mathbf{Z}$ en su dominio, usando la evidencia dada $\\mathbf{Z}=\\mathbf{z}$.\n",
    "2.  Para cada variable X en $\\mathbf{X} \\setminus (\\mathbf{W} \\cup \\mathbf{Z})$, eliminar la variable X mediante marginalización:\n",
    "    1. Hacer el producto de todos los factores que tienen X en su dominio: $\\psi = \\prod_{i \\mid X\\in Dom(\\Phi_i) }\\Phi_i$. \n",
    "    2. Marginalizar X del factor producto obtenido en A: $\\tau = \\sum_X \\psi$.\n",
    "    3. Actualizar la lista de factores quitando los factores que incluyen X y añadiendo el factor marginalizado $\\tau$: $\\mathbf{\\Phi} = (\\mathbf{\\Phi} \\setminus {\\psi}) \\cup \\tau$.\n",
    "3. Multiplicar factores restantes.\n",
    "4. Renormalizar para obtener una distribución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una clase para representar una red bayesiana a partir de la factorización de la distribución conjunta. Nos ayudamos de una serie de funciones auxiliares para normalizar, marginalizar y reducir factores, e implementamos un método para aplicar el algoritmo de eliminación de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNetwork:\n",
    "    \"\"\"\n",
    "    Represents a Bayesian Network via its joint distribution decomposition.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, factor_list):\n",
    "        \"\"\" Construct a network from its factorization. Every factor must have the same\n",
    "            number of dimensions, which coincides with the total number of variables.\"\"\"\n",
    "        \n",
    "        self.factor_list = factor_list\n",
    "        self.variables = np.arange(len(self.factor_list))\n",
    "\n",
    "    def _normalize(self, distribution):\n",
    "        \"\"\" Normalize a distribution so that all values add up to 1. \"\"\"\n",
    "\n",
    "        return distribution / np.sum(distribution)\n",
    "\n",
    "    def _marginal(self, distribution, variables):\n",
    "        \"\"\" Marginalize a distribution for the given list of (indices of) variables. \"\"\"\n",
    "\n",
    "        return np.sum(distribution, axis = tuple(variables), keepdims = True)\n",
    "\n",
    "    def _reduce(self, distribution, variables, asignments, normalize_output = True):\n",
    "        \"\"\" This function receives a distribution, \n",
    "            a list of indices to variables and \n",
    "            a list of assignments to those variables.\n",
    "            \n",
    "            It performs reduction on the specified variables with the given assignments. \"\"\"\n",
    "\n",
    "        reduced = distribution.copy()\n",
    "        for variable, asignment in zip(variables, asignments):\n",
    "            reduced = np.swapaxes(reduced, 0, variable)[[asignment]]\n",
    "            reduced = np.swapaxes(reduced, 0, variable)\n",
    "\n",
    "        return self._normalize(reduced) if normalize_output else reduced\n",
    "    \n",
    "    def _factor_has_var(self, factor, variable):\n",
    "        \"\"\" Return whether a given factor has a specific variable on its domain. \"\"\"\n",
    "\n",
    "        return factor.shape[variable] > 1\n",
    "\n",
    "    def _multi_prod(self, arrays):\n",
    "        \"\"\" Perform the element-wise product of multiple arrays. \"\"\"\n",
    "        \n",
    "        assert(len(arrays) > 0)\n",
    "\n",
    "        res = arrays[0]\n",
    "        for arr in arrays[1:]:\n",
    "            res = res * arr\n",
    "\n",
    "        return res\n",
    "\n",
    "    def VE(self, W, Zs = [], zs = [], order = []):\n",
    "        \"\"\" Implement the variable elimination algorithm.\n",
    "\n",
    "            Input:\n",
    "                * W:       list of desired variables in the output factor.\n",
    "                * Zs:      list with the observed variables.\n",
    "                * zs:      list with the values of the observed variables.\n",
    "                * order:   order in which the variables not in (W U Zs) are processed. \n",
    "                           If empty, an ascending order is assumed. If not empty, the indices\n",
    "                           must be relative to the ones in self.factor_list.\n",
    "            \n",
    "            Ouput:\n",
    "                * Factor representing the joint distribution P(W|Zs=zs).\n",
    "                * The size of the biggest factor processed.\n",
    "        \"\"\"\n",
    "        \n",
    "        factors = self.factor_list.copy()\n",
    "        variables_factors = np.arange(len(factors))\n",
    "\n",
    "        # -- STEP 1: reduce factors that contain any variable in Zs --\n",
    "        for Z, z in zip(Zs, zs):\n",
    "            for i, factor in enumerate(factors):\n",
    "                if self._factor_has_var(factor, Z):\n",
    "                    factors[i] = self._reduce(factor, [Z], [z], False)\n",
    "\n",
    "        # -- STEP 2: eliminate variables via marginalization --\n",
    "        variables_rest = np.setdiff1d(variables_factors, np.union1d(W, Zs))\n",
    "        \n",
    "        # Take the desired ordering into account\n",
    "        if len(order) > 0:\n",
    "            assert((np.unique(order) == np.unique(variables_rest)).all() \n",
    "                      and len(order) == len(variables_rest))\n",
    "            variables_rest = order\n",
    "            \n",
    "        # Process each variable in order\n",
    "        max_size = 0\n",
    "        for X in variables_rest:\n",
    "            # Recover the indices of factors that have X on their domain\n",
    "            factors_X_idx = [i for i in variables_factors if self._factor_has_var(factors[i], X)]\n",
    "            \n",
    "            # Multiply those factors together\n",
    "            psi = self._multi_prod([factors[i] for i in factors_X_idx])\n",
    "            \n",
    "            # Marginalize the product with respect to X\n",
    "            tau = self._marginal(psi, [X])\n",
    "            \n",
    "            # Update the factor list\n",
    "            factors = [factors[i] for i in np.setdiff1d(variables_factors, factors_X_idx)] + [tau]\n",
    "            variables_factors = np.arange(len(factors))\n",
    "            \n",
    "            # Compute size of the processed factor\n",
    "            max_size = max(max_size, np.prod(psi.shape))\n",
    "\n",
    "        # -- STEPS 3 and 4: multiply the remaining factors and normalize --\n",
    "        res = self._normalize(self._multi_prod([*factors]))\n",
    "        \n",
    "        return res, max_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una distribución para probar el algoritmo, basándonos en el siguiente grafo:\n",
    "\n",
    "![estu](img/estu2.png)\n",
    "\n",
    "Las variables representadas son:\n",
    "\n",
    "* **Nota examen (G)**: g0 (sobresaliente), g1 (notable), g2 (aprobado).\n",
    "* **Dificultad examen (D)**: d0 (fácil) y d1 (difícil).\n",
    "* **Inteligencia (I)**: i0 (normal), i1 (alta).\n",
    "* **Nota Selectividad (S)**: s0 (baja), s1 (alta).\n",
    "* **Carta de recomendación (L)**: l0 (regular), l1 (buena).\n",
    "\n",
    "La distribución conjunta es entonces:\n",
    "\n",
    "$$P(I,D, G,L,S) = P(I)P(D)P(G|I,D)P(L|G)P(S|I).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensión -> 0  1  2  3  4\n",
    "# Variable  -> I  D  G  L  S\n",
    "\n",
    "PI = np.array([0.7, 0.3]).reshape((2, 1, 1, 1, 1))\n",
    "PD = np.array([0.6, 0.4]).reshape((1, 2, 1, 1, 1))\n",
    "PG_ID = np.array([0.3, 0.4, 0.3, 0.05, 0.25, 0.7, 0.9, 0.08, 0.02, 0.5, 0.3, 0.2]).reshape((2, 2, 3, 1, 1))\n",
    "PL_G = np.array([0.1, 0.9, 0.4, 0.6, 0.99, 0.01]).reshape((1, 1, 3, 2, 1))\n",
    "PS_I = np.array([0.95, 0.05, 0.2, 0.8]).reshape((2, 1, 1, 1, 2))\n",
    "\n",
    "# Distribución conjunta\n",
    "PIDGLS = PI * PD * PG_ID * PL_G* PS_I\n",
    "\n",
    "# Red bayesiana\n",
    "network = BayesianNetwork([PI, PD, PG_ID, PL_G, PS_I])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos una serie de casos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los casos de prueba son correctos.\n"
     ]
    }
   ],
   "source": [
    "# ¿Cuál es la distribución P(I)?\n",
    "factor, max_size = network.VE([0])\n",
    "assert(np.allclose(np.array([[[[[0.7]]]], [[[[0.3]]]]]), factor))\n",
    "assert(max_size == 12)\n",
    "\n",
    "# Si sabemos que la nota del examen es aprobado, ¿cuál es la distribución de la inteligencia? \n",
    "# P(I|G=g2)\n",
    "factor, max_size = network.VE([0], [2], [2])\n",
    "assert(np.allclose(np.array([[[[[0.92105263]]]], [[[[0.07894737]]]]]), factor))\n",
    "assert(max_size == 4)\n",
    "\n",
    "# Y si además el examen es difícil?\n",
    "# P(I|G=g2,D=d1)\n",
    "factor, max_size = network.VE([0], [1,2], [1,2])\n",
    "assert(np.allclose(np.array([[[[[0.89090909]]]], [[[[0.10909091]]]]]), factor))\n",
    "assert(max_size == 4)\n",
    "\n",
    "print(\"Todos los casos de prueba son correctos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los casos de prueba son correctos.\n"
     ]
    }
   ],
   "source": [
    "# Calcula la distribución P(D)\n",
    "factor, max_size = network.VE([1])\n",
    "assert(np.allclose(np.array([[[[[0.6]]], [[[0.4]]]]]), factor))\n",
    "assert(max_size == 24)\n",
    "\n",
    "# Prob examen si nota es aprobado: P(D|G=g2)\n",
    "factor, max_size = network.VE([1], [2], [2])\n",
    "assert(np.allclose(np.array([[[[[0.37070938]]], [[[0.62929062]]]]]), factor))\n",
    "assert(max_size == 8)\n",
    "\n",
    "# Probabilidad de examen difícil D=d1 dado que G=g2 y S=s1\n",
    "factor, max_size = network.VE([1], [2,4], [2,1])\n",
    "assert(np.allclose(np.array([[[[[0.24044002]]], [[[0.75955998]]]]]), factor))\n",
    "assert(max_size == 4)\n",
    "\n",
    "print(\"Todos los casos de prueba son correctos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente estudiamos cómo influyen unas variables sobre otras, dependiendo de la evidencia que tengamos. En concreto, nos preguntamos si la nota de selectividad (S) influye en la dificultad del examen (D), dependiendo de si conocemos o no su nota (G)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de D:\n",
      " [[[[[0.6]]]\n",
      "\n",
      "\n",
      "  [[[0.4]]]]]\n",
      "\n",
      "Distribución de D|S=1:\n",
      " [[[[[0.6]]]\n",
      "\n",
      "\n",
      "  [[[0.4]]]]]\n",
      "--> No cambia\n",
      "\n",
      "Distribución de D|G=2:\n",
      " [[[[[0.37070938]]]\n",
      "\n",
      "\n",
      "  [[[0.62929062]]]]]\n",
      "--> Cambia\n",
      "\n",
      "Distribución de D|G=2,S=1:\n",
      " [[[[[0.24044002]]]\n",
      "\n",
      "\n",
      "  [[[0.75955998]]]]]\n",
      "--> Cambia\n"
     ]
    }
   ],
   "source": [
    "# P(D)\n",
    "pd, _ = network.VE([1])\n",
    "print(\"Distribución de D:\\n\", pd)\n",
    "\n",
    "# P(D|S=1), no conocemos G\n",
    "pd_s1, _ = network.VE([1], [4], [1])\n",
    "print(\"\\nDistribución de D|S=1:\\n\", pd_s1)\n",
    "if np.allclose(pd_s1, pd):\n",
    "    print(\"--> No cambia\")\n",
    "else:\n",
    "    print(\"--> Cambia\")\n",
    "\n",
    "# P(D|G=2)\n",
    "pd_g2, _ = network.VE([1], [2], [2])\n",
    "print(\"\\nDistribución de D|G=2:\\n\", pd_g2)\n",
    "if np.allclose(pd_g2, pd):\n",
    "    print(\"--> No cambia\")\n",
    "else:\n",
    "    print(\"--> Cambia\")\n",
    "\n",
    "# P(D|G=2,S=1), conocemos G\n",
    "pd_g2s1, _ = network.VE([1], [2, 4], [2, 1])\n",
    "print(\"\\nDistribución de D|G=2,S=1:\\n\", pd_g2s1)\n",
    "if np.allclose(pd_g2s1, pd):\n",
    "    print(\"--> No cambia\")\n",
    "else:\n",
    "    print(\"--> Cambia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "Entrada: training set $\\{(x_n, y_n\\}_{n=1,\\dots,N}$, una función de pérdida diferenciable $L(y,F(x))$ y un número de iteraciones $M$.\n",
    "\n",
    "1. Inicializa el modelo con un clasificador constante\n",
    "$$\n",
    "F_0 (x) = \\argmin_{\\gamma} \\sum_{n=1}^N L(y_n, \\gamma).\n",
    "$$\n",
    "2. Para $m = 1,\\dots,M$:\n",
    "    1. Computa los \"pseudo-residuos\":\n",
    "    $$\n",
    "        r_{n,m} = - \\left[ \\frac{\\partial L(y_n, F(x_n))}{\\partial F(x_n)} \\right]_{F(x) = F_{m-1}(x)} \\quad n = 1,\\dots,N\n",
    "    $$\n",
    "    2. Ajusta un modelo base $h_m(x)$ a los pseudo-residuos, es decir, entrenalo sobre el conjunto $\\{x_n, r_{n,m}\\}_{n=1,\\dots,N}$\n",
    "    3. Computa el multiplicador $\\gamma_m$ que minimiza el error en entrenamiento (paso de Newton-Raphson).\n",
    "    $$\n",
    "    \\gamma_m = \\argmin_\\gamma \\sum_{n=1}^N L(y_n, F_{m-1}(x_n) + \\gamma h_m(x_n)).\n",
    "    $$\n",
    "    4. Actualiza el modelo\n",
    "    $$\n",
    "    F_m(x) F_{m-1}(x) + \\gamma_m h_m(x).\n",
    "    $$\n",
    "3. Devuelve $F_M(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error cuadrático.\n",
    "\n",
    "Creamos la clase que modela los elementos del algoritmo correspondientes al uso del error cuadrático\n",
    "$$\n",
    "L(y, F(x)) = (y - F(x))^2.\n",
    "$$\n",
    "Entre estos elementos se encuentra: el cálculo del estimador inicial $F_0$, el cálculo de residuos, la minimización del paso de Newton-Raphson y el cálculo del propio error.\n",
    "\n",
    "1. **Estimador inicial**. En este caso, el valor del estimador constante inicial coincide con la media:\n",
    "$$\n",
    "\\argmin_\\gamma \\sum_{n=1}^N L(y_n, \\gamma) = \\argmin_\\gamma \\sum_{n=1}^N (y_n - \\gamma)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sum_{n=1}^N L(y_n, \\gamma)}{\\partial \\gamma} = \\sum_{n=1}^N(y_n - \\gamma) = 0 \\implies \\gamma = \\frac{1}{N}\\sum_{n=1}^N y_n.\n",
    "$$\n",
    "2. **Pseudo-residuos**. \n",
    "$$\n",
    "r_{n,m} = - \\left[ \\frac{\\partial L(y_n, F(x_n))}{\\partial F(x_n)} \\right]_{F(x) = F_{m-1}(x)} = -\\left[ \\frac{\\partial (y_n - F(x_n)^2}{\\partial F(x_n)} \\right]_{F(x) = F_{m-1}(x)} = 2(y_n - F_{m-1}(x_n))\n",
    "$$\n",
    "\n",
    "3. **Multiplicador**. Al no poder computar el valor que anula la siguiente derivada, le aplicamos un paso del método de Newton-Rapson:\n",
    "$$\n",
    "f(\\rho) = \\frac{\\partial}{\\partial \\rho} \\sum_{n=1}^N (y_n - F_{m-1}(x_n) - \\rho h_m(x_n))^2\n",
    "$$\n",
    "$$\n",
    "f(\\rho) = \\sum_{n=1}^N -2h_m(x_n)(y_n - F_{m-1}(x_n) - \\rho h_m(x_n))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f'(\\rho) = \\sum_{n=1}^N 2h_m(x_n)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rho^{new} = -\\frac{f(\\rho = 0)}{f'(\\rho = 0)} = \\frac{\\sum_{n=1}^N 2h_m(x_n)(y_n - F_{m-1}(x_n))}{\\sum_{n=1}^N 2h_m(x_n)^2}\n",
    "$$\n",
    "Donde, utilizando que \n",
    "$$\n",
    "h_m(x_n) \\approx r_{n,m} = 2(y_n- F_{m-1}(x_n))\n",
    "$$\n",
    "$$\n",
    "\\rho^{new} = \\frac{\\sum_{n=1}^N 4(y_n- F_{m-1}(x_n))(y_n - F_{m-1}(x_n))}{\\sum_{n=1}^N 4(y_n- F_{m-1}(x_n))^2} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredErrorLoss:\n",
    "    \"\"\" Clase que define elementos para la función cuadrática de perdida \n",
    "        para regresión\"\"\"\n",
    "    \n",
    "    def F0(_, X, y):\n",
    "        \"\"\" Calcula el valor constate que minimiza la salida 'y' \"\"\"\n",
    "        \"\"\" \n",
    "            Calcula el primer sumando del estimador final F0. Al utilizar la función de perdida cuadrática, \n",
    "            este valor coincide con la media de los valores.\n",
    "        \"\"\"\n",
    "        return np.median(y)\n",
    "\n",
    "    def residuos(_, y, F):\n",
    "        \"\"\" Calcula los residuos para un objetivo 'y' y \n",
    "            una salida del modelo F \"\"\"\n",
    "        \"\"\"\n",
    "            Calcula el valor de los pseudo-residuos. \n",
    "        \"\"\"\n",
    "        return np.sign(y - F)\n",
    "\n",
    "    def paso_newton(_, y, residuos, ht):\n",
    "        \"\"\" Recibe el valor a predecir (y), los pseudo-residuos (residuos) \n",
    "            sobre los que se entrena el regresor ht y la salida del nuevo \n",
    "            regresor creado (ht) \"\"\"\n",
    "        \"\"\"\n",
    "            Calcula el multiplicador a partir de y, los residuos y el nuevo estimador de estos ultimos\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def __call__(self, y, F):\n",
    "        \"\"\" Devuelve el valor de la función de pérdida para un objetivo 'y' y \n",
    "            una salida del modelo F \"\"\"\n",
    "        return (y-F)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error logístico\n",
    "\n",
    "Creamos la clase que modela los elementos del algoritmo correspondientes al uso del error cuadrático\n",
    "$$\n",
    "L(y, F(x)) = \\log(1 + \\exp(-2yF)), \\quad y \\in \\{-1,1\\}.\n",
    "$$\n",
    "Entre estos elementos se encuentra: el cálculo del estimador inicial $F_0$, el cálculo de residuos, la minimización del paso de Newton-Raphson y el cálculo del propio error.\n",
    "\n",
    "1. **Estimador inicial**. En este caso, el valor del estimador constante inicial coincide con la media:\n",
    "$$\n",
    "\\argmin_\\gamma \\sum_{n=1}^N L(y_n, \\gamma) = \\argmin_\\gamma \\sum_{n=1}^N \\log(1 + \\exp(-2y_n \\gamma))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sum_{n=1}^N L(y_n, \\gamma)}{\\partial \\gamma} = \\frac{-2y_n \\exp(-2y\\gamma)}{1 + \\exp(-2y_n \\gamma))} = 0 \\implies \\gamma = \\frac{1}{N}\\sum_{n=1}^N y_n.\n",
    "$$\n",
    "2. **Pseudo-residuos**. \n",
    "$$\n",
    "r_{n,m} = - \\left[ \\frac{\\partial L(y_n, F(x_n))}{\\partial F(x_n)} \\right]_{F(x) = F_{m-1}(x)} = -\\left[ \\frac{\\partial (y_n - F(x_n)^2}{\\partial F(x_n)} \\right]_{F(x) = F_{m-1}(x)} = 2(y_n - F_{m-1}(x_n))\n",
    "$$\n",
    "\n",
    "3. **Multiplicador**. Al no poder computar el valor que anula la siguiente derivada, le aplicamos un paso del método de Newton-Rapson:\n",
    "$$\n",
    "f(\\rho) = \\frac{\\partial}{\\partial \\rho} \\sum_{n=1}^N (y_n - F_{m-1}(x_n) - \\rho h_m(x_n))^2\n",
    "$$\n",
    "$$\n",
    "f(\\rho) = \\sum_{n=1}^N -2h_m(x_n)(y_n - F_{m-1}(x_n) - \\rho h_m(x_n))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLoss:\n",
    "    \"\"\" Clase que define elementos para la función logística de perdida \n",
    "        para clasificación de dos clases {-1, +1} \"\"\"\n",
    "    \n",
    "    def F0(_, X, y):\n",
    "        \"\"\" Calcula el valor constate que minimiza la salida 'y' \"\"\"\n",
    "        return 0.5*np.log((1+np.mean(y))/(1-np.mean(y)))\n",
    "\n",
    "    def residuos(_, y, F):\n",
    "        \"\"\" Calcula los residuos para un objetivo 'y' y \n",
    "            una salida del modelo F \"\"\"\n",
    "        return 2*y/(1 + np.exp(2*y*F))\n",
    "\n",
    "    def paso_newton(_, y, residuos, ht):\n",
    "        \"\"\" Recibe el valor a predecir (y), los pseudo-residuos (residuos) \n",
    "            sobre los que se entrena el regresor ht y la salida del nuevo \n",
    "            regresor creado (ht) \"\"\"\n",
    "        return np.sum( residuos/(np.abs(residuos)*(2 - np.abs(residuos))) )\n",
    "\n",
    "    def __call__(self,y, F):\n",
    "        \"\"\" Devuelve el valor de la función de pérdida para un objetivo 'y' y \n",
    "            una salida del modelo F \"\"\"\n",
    "        return np.log(1+np.exp(-2.0*y*F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GBCasero:\n",
    "    def __init__(self, n_estimators=101, loss=SquaredErrorLoss(), reg = True, eta=0.1, depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self._estimators  = []\n",
    "        self._multipliers = []\n",
    "        self.depth        = depth\n",
    "        self.eta          = eta\n",
    "        self.loss         = loss\n",
    "        self.reg          = reg\n",
    "        \n",
    "    def _transform_output(self, y):\n",
    "        y =  (1/(1 + np.exp(-2*y)) > 0.5).astype(int)\n",
    "        y[ y == 0] = -1\n",
    "        return y\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "          Inicializa GB \n",
    "        \"\"\"\n",
    "        self.F0 = self.loss.F0(X, y)\n",
    "        F = self.F0*np.ones(len(y))\n",
    "        print(\"F:\", F)\n",
    "\n",
    "    \n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth = self.depth)\n",
    "            \"\"\"\n",
    "              Rellenar esta parte para implementar GB        \n",
    "            \"\"\"\n",
    "            r = self.loss.residuos(y, F)\n",
    "            tree.fit(X,r)\n",
    "            pred = tree.predict(X)\n",
    "            mult = self.loss.paso_newton(y, r, pred)\n",
    "            self._multipliers.append(mult)\n",
    "            self._estimators.append(tree)\n",
    "            \n",
    "            F += mult*pred\n",
    "            print(\"Residuos: \", r)\n",
    "            print(\"F:\", F)\n",
    "            print(\"Mult:\", mult)\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def predict(self,X):\n",
    "        \"\"\" Combinamos los valores de pesos y hs para obtener la 'salida', y:\n",
    "               * Para regresión se devuelve 'salida'\n",
    "               * Para clasificación de 2 clases se calcula la probabilidad\n",
    "                  con una sigmoidal (1 / (1 + np.exp(-2*salida))) y se \n",
    "                  devuelve la clase más probable \"\"\"\n",
    "        salida = self.F0\n",
    "        for i in range(self.n_estimators):\n",
    "            salida += self._multipliers[i]*self._estimators[i].predict(X)\n",
    "                \n",
    "        if self.reg == True:\n",
    "            return salida\n",
    "        \n",
    "        return self._transform_output(salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModel(x,y,clase,clf,title=\"\"):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    x_min, x_max = x.min() - .2, x.max() + .2\n",
    "    y_min, y_max = y.min() - .2, y.max() + .2\n",
    "    hx = (x_max - x_min)/100.\n",
    "    hy = (y_max - y_min)/100.\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
    "\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    elif hasattr(clf, \"predict_proba\"):\n",
    "        z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    else:\n",
    "        z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    z = z.reshape(xx.shape)\n",
    "    print(z)\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    plt.contourf(xx, yy, z, cmap=cm, alpha=.8)\n",
    "    plt.contour(xx, yy, z, [0.5], linewidths=[2], colors=['k'])\n",
    "\n",
    "    if clase is not None:\n",
    "        plt.scatter(x[clase==0], y[clase==0], c='#FF0000')\n",
    "        plt.scatter(x[clase==1], y[clase==1], c='#0000FF')\n",
    "    else:\n",
    "        plt.plot(x,y,'g', linewidth=3)\n",
    "        \n",
    "    plt.gca().set_xlim(xx.min(), xx.max())\n",
    "    plt.gca().set_ylim(yy.min(), yy.max())\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet(n,model,ymargin,noise=None,output_boundary=False):\n",
    "    x = np.random.rand(n,1)*2.0*np.pi\n",
    "    xbnd = np.linspace(0,2.0*np.pi,100)\n",
    "\n",
    "    if model == 'sine':\n",
    "        y = (np.random.rand(n,1) - 0.5)*2.2\n",
    "        c = y > np.sin(x)\n",
    "        ybnd = np.sin(xbnd)\n",
    "    elif model == 'linear':\n",
    "        y = np.random.rand(n,1)*2.0*np.pi\n",
    "        c = y > x\n",
    "        ybnd = xbnd\n",
    "    elif model == 'square':\n",
    "        y = np.random.rand(n,1)*4.0*np.pi*np.pi\n",
    "        c = y > x*x\n",
    "        ybnd = xbnd*xbnd\n",
    "    else:\n",
    "        y = np.random.rand(n,1)*2.0*np.pi\n",
    "        c = y > x\n",
    "        ybnd = xbnd\n",
    "    \n",
    "    y[c == True] = y[c == True] + ymargin\n",
    "    y[c == False] = y[c == False] - ymargin\n",
    "    \n",
    "    if noise is not None:\n",
    "        y = y + noise * np.random.randn(n,1)\n",
    "        x = x + noise * np.random.randn(n,1)\n",
    "\n",
    "    if output_boundary == True:\n",
    "        return x, y, (c*1).ravel(), xbnd, ybnd\n",
    "    else:\n",
    "        return x, y, (c*1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(x,y,c,style0,style1,title=''):\n",
    "    plt.scatter(x[c==0],y[c==0],**style0)\n",
    "    plt.scatter(x[c==1],y[c==1],**style1)\n",
    "    plt.grid(True)\n",
    "    plt.axis([x.min()-0.2, x.max()+0.2, y.min()-0.2, y.max()+0.2])\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUYklEQVR4nO3dYYxd513n8e/fSQw7catgpQypHc8UKWKFqLbNRGlRu2y8kMoJ1QaqZpVqCKhSGVU0VQGV3eyOVNgXliptVa1auqlGaZYUD8xWlAqrsZqWMFbpi0Jstm2ShoCV9biOgwwYChO/cNP58+LcwWP73uMZz733uefc70e6uvc899x7/48mOT+fc57znMhMJEnqZUfpAiRJo82gkCTVMigkSbUMCklSLYNCklTr+tIFDMLNN9+c09PTV7S/8sor3HjjjcMvaMDa2K829gnsV5O0sU/Qu1/Hjx//u8x8XbfPtDIopqenOXbs2BXtR48e5a677hp+QQPWxn61sU9gv5qkjX2C3v2KiJVen/HQkySplkEhSaplUEiSahkUkqRaBoUkqZZBIUmqZVBIkmoZFJKkWgaFJKmWQSFJqmVQSJJqGRSSpFoGhSSp1tgHxeIiTE/Djh3V8+Ji6YokabS0cprxzVpchLk5OH++Wl5ZqZYBZmfL1SVJo2Ss9yjm5y+GxLrz56t2SVJlrIPi1KmttUvSOBrroNi3b2vtkjSOxjooDh6EiYlL2yYmqnZJUqVoUETEYxFxNiKe7fF+RMQnIuJERHwrIm7v5+/PzsLCAkxNQUT1vLDgiWxJ2qj0HsXvAAdq3r8HuK3zmAMe6XcBs7Nw8iSsrVXPhoR0dZcPKz93rnRFGqSiQZGZXwXq/hO7D/hsVr4O3BQRtwynOkndrA8rX1mBzOp5ZcVrkNosMrNsARHTwBcz8ye6vPdF4KOZ+bXO8lPAf83MY13WnaPa62BycnJmaWnpit9aXV1l165d/e3ACGhjv9rYJ2hHv555Bi5cuLRt795Vzp7dxRvfWKamQWjD36qbXv3av3//8cy8o+uHMrPoA5gGnu3x3hPA2zcsPwXMXO07Z2Zmspvl5eWu7U3Xxn61sU+Z7ehXRGa1L3Hx8bGPLWdE6cr6qw1/q2569Qs4lj22qaXPUVzNaeDWDct7gTOFapGEw8rH0agHxWHgFzujn94KfDczXy5dlDTOug0r37HDYeVtVnSup4j4feAu4OaIOA38JnADQGZ+GjgC3AucAM4D7y1TqaR16yMD5+erWQz27auGlr/rXWXr0uAUDYrMfM9V3k/gA0MqR9Imzc5eOpT86NFipWgIRv3QkySpMINCklTLoJAk1TIoJEm1DApJUi2DQpJUy6CQJNUyKCRJtQwKSVItg0KSVMugkCTVMigkSbUMikIuv+ewt5GUNKqKzh47rtbvOXz+fLW8slItw6UzckrSKHCPooD5+Yshse78+apdkkaNQVHAqVNba5ekkgyKArznsKQmMSgK6HbP4YkJ7zksaTSNfVCUGH00OwsLC9V9hiOq54UFT2RLGk1jPeqp5Oijy+85LEmjaqz3KBx9JElXN9ZB4egjSbq6sQ6KRo8+2nhy5ZlnvLRb0sCMdVA0dvTR+smVlRXIhAsXqmXDQtIAjHVQNHb0kSdXJA3RWI96goaOPvLkiqQhGus9isZq9MkVSU1jUDRRY0+uSGoig6KJLj+5snNnQ06uSGoig6KpZmfh5ElYW4M3vtGQkDQwBkVbeQs9SX0y9qOeWslb6EnqI/co2sjrLCT1kUHRRl5nIamPDIo28joLSX1kULSR11lI6qOiQRERByLihYg4EREPd3n/roj4bkR8o/P4SIk6G6exk1hJGkXFRj1FxHXAp4C7gdPA0xFxODO/fdmqf5qZ7xx6gU3XyEmsJI2iknsUdwInMvPFzLwALAH3FaxHkkbCqF0GFZlZ5ocj3g0cyMz3dZYfBN6SmQ9tWOcu4PNUexxngA9n5nM9vm8OmAOYnJycWVpaumKd1dVVdu3axblz8NJL1W0cdu6EPXtg9+7+9m+Y1vvVJm3sE9ivJinVp3Pnqkuf1tYutu3YUR1B7sd2qle/9u/ffzwz7+j6ocws8gDuBx7dsPwg8MnL1nktsKvz+l7grzfz3TMzM9nN8vJyHjqUOTGRWd3xp3pMTGQeOtT1I42wvLxcuoS+a2OfMu1Xk5Tq09TUpdun9cfUVH++v1e/gGPZY5ta8tDTaeDWDct7qfYa/lVm/lNmrnZeHwFuiIibt/OjXosmaZSN4mVQJYPiaeC2iHhDROwEHgAOb1whIn4kIqLz+k6qev9+Oz86in8ESVo3ipdBFQuKzHwVeAh4Enge+FxmPhcR74+I93dWezfwbER8E/gE8EBnF+majeIfQZLWjeJlUEUnBewcTjpyWdunN7z+beC3+/mbBw9eOl8elP8jSNK69VHt8/PVkY59+6rtU8nR7mM3e+wo/hEkaaNRuwxq7IICRu+PIEmjzLmeJEm1DApJUi2DQpJUy6CQJNUyKCRJtQwKSVItg0KSVMugkCTVMigkSbUMCklSLYNCklTLoJAk1TIoJEm1DApJGrDFRZiehh07qufFxdIVbY1BIQ1Z0zca2prFxepmaSsrkFk9z8016+9uUEhDdO5c8zca2pr5+UvvqAnV8vx8mXquhUEhDdFLLzV/o6GtOXVqa+2jyKCQhujChe7tTdpoaGv27dta+ygyKKQh2rmze3uTNhramoMHYWLi0raJiaq9KQwKaYj27Gn+RkNbMzsLCwswNQUR1fPCQtXeFAaFNES7dzd/o6Gtm52Fkydhba16btrf+/rSBUjjZna2eRsKjTf3KCRJtQyKNjh3ziu4JA2Mh56abnERzp6trtyCi1dwgcc3JPWFexRNNz9fnSHbyCu4JPWRQdF0bbjsU9JIMyiarg2XfUoaaQZF0x08WJ3E3sgruCT1kUHRdLOz1VVbXsElaUB6jnqKiCPAr2TmyeGVo2uye3d1uackDUDdHsXvAF+OiPmIuGFI9UiSRkzPPYrM/FxEPAF8BDgWEb8LrG14/+NDqE+SVNjVzlF8D3gF+AHgNZc9ti0iDkTECxFxIiIe7vJ+RMQnOu9/KyJu78fvSpI2r+4cxQHg48Bh4PbMPN9r3WsREdcBnwLuBk4DT0fE4cz89obV7gFu6zzeAjzSeZYkDUndFB7zwP2Z+dyAfvtO4ERmvggQEUvAfcDGoLgP+GxmJvD1iLgpIm7JzJcHVJMk6TJ15yj+/YB/ew/wnQ3Lp7lyb6HbOnuAK4IiIuaAOYDJyUmOHj16xQ+urq52bW+6NvarjX0C+9UkbewTXGO/MrPIA7gfeHTD8oPAJy9b5wng7RuWnwJmrvbdMzMz2c3y8nLX9qZrY7/a2KdM+9Uk/ejToUOZU1OZEdXzoUPb/spt69Uv4Fj22KaWnD32NHDrhuW9wJlrWEeSRs7iYjWR8/nO2d0mT+xc8srsp4HbIuINEbETeIDqxPlGh4Ff7Ix+eivw3fT8hKQGmJ+/GBLrmjqxc7E9isx8NSIeAp4ErgMey8znIuL9nfc/DRwB7gVOAOeB95aqV5K2ok0TOxe9cVFmHqEKg41tn97wOoEPDLsuSdquffsu3k/s8vamcVJASRqAgweriZw3aurEzgaFJA3A7Gw1kXMbJnb2ntmSNCCzs80Mhsu5RyFJqmVQSJJqGRSSpFoGhSSplkEhSaplUEiSahkUkqRaBoWkRlpchOlp2LGjel5cLF1Re3nBnaTGadMU3k3gHoWkxmnTFN5NYFBIapw2TeHdBAaFpMbpNVV3E6fwbgKDQlLjtGkK7yYwKCQ1Tpum8G4CRz1JaqS2TOHdBO5RSJJqGRSSpFoGhSSplkEhSaplUEj95iREahmDojQ3Ku2yPgnRygpkXpyEyL+rGsygKMmNSvs4CZFayKAoyY1K+zgJkVrIoCjJjUr7OAmRWsigKMmNSvs4CdHAeVpv+AyKktyotI+TEA2Up/XKMChKcqPSTrOzcPIkrK1Vz/49+8bTemU4KWBpzmwmbZqn9cpwj0JSY3harwyDQlJjeFqvDINCUmN4Wq8Mz1FIahRP6w1fkaCIiN3A/wWmgZPAf87Mf+iy3kngn4HvA69m5h3Dq1KSBOUOPT0MPJWZtwFPdZZ72Z+ZbzIkJKmMUkFxH/B45/XjwM8VqkOSdBWlgmIyM18G6Dz/cI/1EvhyRByPiLmhVSdJ+leRmYP54og/Bn6ky1vzwOOZedOGdf8hM3+oy3e8PjPPRMQPA18BPpiZX+3xe3PAHMDk5OTM0tLSFeusrq6ya9eua+nOSGtjv9rYJ7BfTdLGPkHvfu3fv/94z0P8mTn0B/ACcEvn9S3AC5v4zG8BH97M98/MzGQ3y8vLXdubro39amOfMu1Xk7SxT5m9+wUcyx7b1FKHng4Dv9R5/UvAH12+QkTcGBGvWX8NvAN4dmgVSpKAcucoPgrcHRF/DdzdWSYiXh8RRzrrTAJfi4hvAn8OPJGZXypSrYZnfQ7p48edQ1oaEUWuo8jMvwd+ukv7GeDezusXgX835NJU0voc0uvTg67PIQ1eYSUV5BQeGh3OIS2NJINCo8M5pKWRZFBodDiHtDSSDAqNDueQlkaSQaHRsXEOaXAOaWlEOM24Rsv6HNJHj1b3m5ZUnHsUkqRaBoUkqZZBIUmqZVBIkmoZFJKkWgaFJKmWQSFJqmVQSJJqGRSSpFoGhSSplkEhSaplUEiSahkUkqRaBoUkqZZBIUmqZVBIkmoZFJKkWgaF1MviIkxPw44d1fPiYumKpCK8FarUzeIizM3B+fPV8spKtQzew1tjxz0KqZv5+Yshse78+apdGjMGhdTNqVNba5dazKCQutm3b2vtUosZFFI3Bw/CxMSlbRMTVbs0ZgwKqZvZWVhYgKkpiKieFxb6cyLb0VRqGEc9Sb3MzvZ/hNO5c46mUuO4RyEN00svOZpKjWNQSMN04UL3dkdTaYQZFNIw7dzZvd3RVBphBoU0THv2OJpKw9HHQRMGhTRMu3cPbjSVtG59CpqVFci8OGjiGsOiSFBExP0R8VxErEXEHTXrHYiIFyLiREQ8PMwapYGZnYWTJ2FtrXo2JNRvfZ6CptQexbPAu4Cv9lohIq4DPgXcA/w48J6I+PHhlCdJDdbnKWiKBEVmPp+ZL1xltTuBE5n5YmZeAJaA+wZfnSQ1XJ+noInM3EY12xMRR4EPZ+axLu+9GziQme/rLD8IvCUzH+rxXXPAHMDk5OTM0tLSFeusrq6ya9eu/nVgRLSxX23sE9ivJml0n86dq85LrK1dbNuxA6amWN25s2u/9u/ffzwzu58KyMyBPIA/pjrEdPnjvg3rHAXu6PH5+4FHNyw/CHxyM789MzOT3SwvL3dtb7o29quNfcq0X03S+D4dOpQ5NZUZUT0fOpSZvfsFHMse29SBTeGRmT+zza84Ddy6YXkvcGab3ylJ46GPU9CM8vDYp4HbIuINEbETeAA4XLgmSRo7pYbH/nxEnAZ+EngiIp7stL8+Io4AZOarwEPAk8DzwOcy87kS9UrSOCsye2xmfgH4Qpf2M8C9G5aPAEeGWJok6TKjfOhJkjQCDApJUi2DQpJUy6CQJNUyKCRJtQwKSVItg0KSVMugkCTVMigkSbUMCklSraL3oxiUiPhbYKXLWzcDfzfkcoahjf1qY5/AfjVJG/sEvfs1lZmv6/aBVgZFLxFxLHvdmKPB2tivNvYJ7FeTtLFPcG398tCTJKmWQSFJqjVuQbFQuoABaWO/2tgnsF9N0sY+wTX0a6zOUUiStm7c9igkSVtkUEiSao1FUETEgYh4ISJORMTDpevpl4h4LCLORsSzpWvpl4i4NSKWI+L5iHguIj5UuqZ+iIgfjIg/j4hvdvr1P0rX1C8RcV1E/L+I+GLpWvolIk5GxDMR8Y2IOFa6nn6JiJsi4g8i4i87/4/95KY+1/ZzFBFxHfBXwN3AaeBp4D2Z+e2ihfVBRPwUsAp8NjN/onQ9/RARtwC3ZOZfRMRrgOPAzzX97xURAdyYmasRcQPwNeBDmfn1wqVtW0T8OnAH8NrMfGfpevohIk4Cd2Rmqy64i4jHgT/NzEcjYicwkZn/eLXPjcMexZ3Aicx8MTMvAEvAfYVr6ovM/CpwrnQd/ZSZL2fmX3Re/zPwPLCnbFXbl5XVzuINnUfj/5UWEXuBnwUeLV2L6kXEa4GfAj4DkJkXNhMSMB5BsQf4zobl07RgwzMOImIaeDPwZ4VL6YvOIZpvAGeBr2RmG/r1v4D/AqwVrqPfEvhyRByPiLnSxfTJjwJ/C/yfzqHCRyPixs18cByCIrq0Nf5fcm0XEbuAzwO/mpn/VLqefsjM72fmm4C9wJ0R0ejDhRHxTuBsZh4vXcsAvC0zbwfuAT7QOczbdNcDtwOPZOabgVeATZ2zHYegOA3cumF5L3CmUC3ahM4x/M8Di5n5h6Xr6bfO7v5R4EDZSrbtbcB/6hzPXwL+Y0QcKltSf2Tmmc7zWeALVIewm+40cHrDnuwfUAXHVY1DUDwN3BYRb+icvHkAOFy4JvXQOen7GeD5zPx46Xr6JSJeFxE3dV7/G+BngL8sWtQ2ZeZ/y8y9mTlN9f/Vn2TmLxQua9si4sbOQAo6h2beATR+ZGFm/g3wnYj4sU7TTwObGiRy/cCqGhGZ+WpEPAQ8CVwHPJaZzxUuqy8i4veBu4CbI+I08JuZ+ZmyVW3b24AHgWc6x/MB/ntmHilXUl/cAjzeGYW3A/hcZrZmOGnLTAJfqP7NwvXA72Xml8qW1DcfBBY7/2h+EXjvZj7U+uGxkqTtGYdDT5KkbTAoJEm1DApJUi2DQpJUy6CQJNUyKKQB68yI+/8jYndn+Yc6y1Ola5M2w6CQBiwzvwM8Any00/RRYCEzV8pVJW2e11FIQ9CZluQ48Bjwy8CbO7MZSyOv9VdmS6MgM78XEb8BfAl4hyGhJvHQkzQ89wAvA42eNVbjx6CQhiAi3kR1l8W3Ar/WuZOf1AgGhTRgnRlxH6G6t8Yp4H8CHytblbR5BoU0eL8MnMrMr3SW/zfwbyPiPxSsSdo0Rz1Jkmq5RyFJqmVQSJJqGRSSpFoGhSSplkEhSaplUEiSahkUkqRa/wKiBvmr1+RJ6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Problem data:\n",
    "np.random.seed(11)\n",
    "import matplotlib.pyplot as plt\n",
    "n = 20\n",
    "model = 'sine'\n",
    "ymargin = 0.\n",
    "noise = 0.0             # <========= Modifica este valor 0 ó 0.3, (antes responde a las cuestiones de arriba)\n",
    "x1, x2, ytrain, xbnd, ybnd = createDataSet(n, model, ymargin, noise, True)\n",
    "x1test, x2test, ytest = createDataSet(n*10, model, ymargin, noise)\n",
    "plotData(x1,x2,ytrain,{'c':'#FF0000'},{'c':'#0000FF'})\n",
    "Xtrain = np.concatenate((x1, x2), axis = 1)\n",
    "Xtest = np.concatenate((x1test, x2test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -8.881784197001252e-16\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -2.220446049250313e-15\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -1.2878587085651816e-14\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -6.439293542825908e-14\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -3.2374103398069565e-13\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -1.6178169914837781e-12\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -8.08819677899919e-12\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -4.044187207341565e-11\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -2.0220936036707826e-10\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -1.0110463577461815e-09\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033534 0.10033535 0.10033534 0.10033535 0.10033535\n",
      " 0.10033534 0.10033534 0.10033535 0.10033534 0.10033534 0.10033534\n",
      " 0.10033534 0.10033534 0.10033535 0.10033534 0.10033534 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -5.055235341444586e-09\n",
      "Residuos:  [-1.10000001  0.90000001 -1.10000001  0.90000001 -1.10000001 -1.10000001\n",
      "  0.90000001  0.90000001 -1.10000001  0.90000001  0.90000001  0.90000001\n",
      "  0.90000001  0.90000001 -1.10000001  0.90000001  0.90000001 -1.10000001\n",
      " -1.10000001 -1.10000001]\n",
      "F: [0.10033538 0.10033532 0.10033538 0.10033532 0.10033538 0.10033538\n",
      " 0.10033532 0.10033532 0.10033538 0.10033532 0.10033532 0.10033532\n",
      " 0.10033532 0.10033532 0.10033538 0.10033532 0.10033532 0.10033538\n",
      " 0.10033538 0.10033538]\n",
      "Mult: -2.527617493086609e-08\n",
      "Residuos:  [-1.10000003  0.90000003 -1.10000003  0.90000003 -1.10000003 -1.10000003\n",
      "  0.90000003  0.90000003 -1.10000003  0.90000003  0.90000003  0.90000003\n",
      "  0.90000003  0.90000003 -1.10000003  0.90000003  0.90000003 -1.10000003\n",
      " -1.10000003 -1.10000003]\n",
      "F: [0.10033552 0.10033521 0.10033552 0.10033521 0.10033552 0.10033552\n",
      " 0.10033521 0.10033521 0.10033552 0.10033521 0.10033521 0.10033521\n",
      " 0.10033521 0.10033521 0.10033552 0.10033521 0.10033521 0.10033552\n",
      " 0.10033552 0.10033552]\n",
      "Mult: -1.2638087865113334e-07\n",
      "Residuos:  [-1.10000017  0.90000014 -1.10000017  0.90000014 -1.10000017 -1.10000017\n",
      "  0.90000014  0.90000014 -1.10000017  0.90000014  0.90000014  0.90000014\n",
      "  0.90000014  0.90000014 -1.10000017  0.90000014  0.90000014 -1.10000017\n",
      " -1.10000017 -1.10000017]\n",
      "F: [0.10033622 0.10033464 0.10033622 0.10033464 0.10033622 0.10033622\n",
      " 0.10033464 0.10033464 0.10033622 0.10033464 0.10033464 0.10033464\n",
      " 0.10033464 0.10033464 0.10033622 0.10033464 0.10033464 0.10033622\n",
      " 0.10033622 0.10033622]\n",
      "Mult: -6.319045540159607e-07\n",
      "Residuos:  [-1.10000086  0.9000007  -1.10000086  0.9000007  -1.10000086 -1.10000086\n",
      "  0.9000007   0.9000007  -1.10000086  0.9000007   0.9000007   0.9000007\n",
      "  0.9000007   0.9000007  -1.10000086  0.9000007   0.9000007  -1.10000086\n",
      " -1.10000086 -1.10000086]\n",
      "F: [0.10033969 0.10033179 0.10033969 0.10033179 0.10033969 0.10033969\n",
      " 0.10033179 0.10033179 0.10033969 0.10033179 0.10033179 0.10033179\n",
      " 0.10033179 0.10033179 0.10033969 0.10033179 0.10033179 0.10033969\n",
      " 0.10033969 0.10033969]\n",
      "Mult: -3.159526774876298e-06\n",
      "Residuos:  [-1.1000043   0.90000352 -1.1000043   0.90000352 -1.1000043  -1.1000043\n",
      "  0.90000352  0.90000352 -1.1000043   0.90000352  0.90000352  0.90000352\n",
      "  0.90000352  0.90000352 -1.1000043   0.90000352  0.90000352 -1.1000043\n",
      " -1.1000043  -1.1000043 ]\n",
      "F: [0.10035707 0.10031758 0.10035707 0.10031758 0.10035707 0.10035707\n",
      " 0.10031758 0.10031758 0.10035707 0.10031758 0.10031758 0.10031758\n",
      " 0.10031758 0.10031758 0.10035707 0.10031758 0.10031758 0.10035707\n",
      " 0.10035707 0.10035707]\n",
      "Mult: -1.579773389304151e-05\n",
      "Residuos:  [-1.1000215   0.90001759 -1.1000215   0.90001759 -1.1000215  -1.1000215\n",
      "  0.90001759  0.90001759 -1.1000215   0.90001759  0.90001759  0.90001759\n",
      "  0.90001759  0.90001759 -1.1000215   0.90001759  0.90001759 -1.1000215\n",
      " -1.1000215  -1.1000215 ]\n",
      "F: [0.10044396 0.10024648 0.10044396 0.10024648 0.10044396 0.10044396\n",
      " 0.10024648 0.10024648 0.10044396 0.10024648 0.10024648 0.10024648\n",
      " 0.10024648 0.10024648 0.10044396 0.10024648 0.10024648 0.10044396\n",
      " 0.10044396 0.10044396]\n",
      "Mult: -7.89911701937207e-05\n",
      "Residuos:  [-1.10010753  0.90008798 -1.10010753  0.90008798 -1.10010753 -1.10010753\n",
      "  0.90008798  0.90008798 -1.10010753  0.90008798  0.90008798  0.90008798\n",
      "  0.90008798  0.90008798 -1.10010753  0.90008798  0.90008798 -1.10010753\n",
      " -1.10010753 -1.10010753]\n",
      "F: [0.10087852 0.09989093 0.10087852 0.09989093 0.10087852 0.10087852\n",
      " 0.09989093 0.09989093 0.10087852 0.09989093 0.09989093 0.09989093\n",
      " 0.09989093 0.09989093 0.10087852 0.09989093 0.09989093 0.10087852\n",
      " 0.10087852 0.10087852]\n",
      "Mult: -0.0003950183779846128\n",
      "Residuos:  [-1.10053772  0.90043999 -1.10053772  0.90043999 -1.10053772 -1.10053772\n",
      "  0.90043999  0.90043999 -1.10053772  0.90043999  0.90043999  0.90043999\n",
      "  0.90043999  0.90043999 -1.10053772  0.90043999  0.90043999 -1.10053772\n",
      " -1.10053772 -1.10053772]\n",
      "F: [0.10305391 0.09811107 0.10305391 0.09811107 0.10305391 0.10305391\n",
      " 0.09811107 0.09811107 0.10305391 0.09811107 0.09811107 0.09811107\n",
      " 0.09811107 0.09811107 0.10305391 0.09811107 0.09811107 0.10305391\n",
      " 0.10305391 0.10305391]\n",
      "Mult: -0.0019766561784559933\n",
      "Residuos:  [-1.10269064  0.90220252 -1.10269064  0.90220252 -1.10269064 -1.10269064\n",
      "  0.90220252  0.90220252 -1.10269064  0.90220252  0.90220252  0.90220252\n",
      "  0.90220252  0.90220252 -1.10269064  0.90220252  0.90220252 -1.10269064\n",
      " -1.10269064 -1.10269064]\n",
      "F: [0.11399539 0.08915894 0.11399539 0.08915894 0.11399539 0.11399539\n",
      " 0.08915894 0.08915894 0.11399539 0.08915894 0.08915894 0.08915894\n",
      " 0.08915894 0.08915894 0.11399539 0.08915894 0.08915894 0.11399539\n",
      " 0.11399539 0.11399539]\n",
      "Mult: -0.00992252767119961\n",
      "Residuos:  [-1.11350415  0.91107656 -1.11350415  0.91107656 -1.11350415 -1.11350415\n",
      "  0.91107656  0.91107656 -1.11350415  0.91107656  0.91107656  0.91107656\n",
      "  0.91107656  0.91107656 -1.11350415  0.91107656  0.91107656 -1.11350415\n",
      " -1.11350415 -1.11350415]\n",
      "F: [0.17035153 0.04304797 0.17035153 0.04304797 0.17035153 0.17035153\n",
      " 0.04304797 0.04304797 0.17035153 0.04304797 0.04304797 0.04304797\n",
      " 0.04304797 0.04304797 0.17035153 0.04304797 0.04304797 0.17035153\n",
      " 0.17035153 0.17035153]\n",
      "Mult: -0.050611525146961434\n",
      "Residuos:  [-1.16872259  0.9569786  -1.16872259  0.9569786  -1.16872259 -1.16872259\n",
      "  0.9569786   0.9569786  -1.16872259  0.9569786   0.9569786   0.9569786\n",
      "  0.9569786   0.9569786  -1.16872259  0.9569786   0.9569786  -1.16872259\n",
      " -1.16872259 -1.16872259]\n",
      "F: [ 0.49809234 -0.22531421  0.49809234 -0.22531421  0.49809234  0.49809234\n",
      " -0.22531421 -0.22531421  0.49809234 -0.22531421 -0.22531421 -0.22531421\n",
      " -0.22531421 -0.22531421  0.49809234 -0.22531421 -0.22531421  0.49809234\n",
      "  0.49809234  0.49809234]\n",
      "Mult: -0.28042651779865535\n",
      "Residuos:  [-1.46061556  1.22157727 -1.46061556  1.22157727 -1.46061556 -1.46061556\n",
      "  1.22157727  1.22157727 -1.46061556  1.22157727  1.22157727  1.22157727\n",
      "  1.22157727  1.22157727 -1.46061556  1.22157727  1.22157727 -1.46061556\n",
      " -1.46061556 -1.46061556]\n",
      "F: [ 4.2293045  -3.34589167  4.2293045  -3.34589167  4.2293045   4.2293045\n",
      " -3.34589167 -3.34589167  4.2293045  -3.34589167 -3.34589167 -3.34589167\n",
      " -3.34589167 -3.34589167  4.2293045  -3.34589167 -3.34589167  4.2293045\n",
      "  4.2293045   4.2293045 ]\n",
      "Mult: -2.5545477327505313\n",
      "Residuos:  [-1.99957596  1.99752094 -1.99957596  1.99752094 -1.99957596 -1.99957596\n",
      "  1.99752094  1.99752094 -1.99957596  1.99752094  1.99752094  1.99752094\n",
      "  1.99752094  1.99752094 -1.99957596  1.99752094  1.99752094 -1.99957596\n",
      " -1.99957596 -1.99957596]\n",
      "F: [ 33571.22146023 -33535.84040243  33571.22146023 -33535.84040243\n",
      "  33571.22146023  33571.22146023 -33535.84040243 -33535.84040243\n",
      "  33571.22146023 -33535.84040243 -33535.84040243 -33535.84040243\n",
      " -33535.84040243 -33535.84040243  33571.22146023 -33535.84040243\n",
      " -33535.84040243  33571.22146023  33571.22146023  33571.22146023]\n",
      "Mult: -16787.055301027103\n",
      "Residuos:  [-2.  2. -2.  2. -2. -2.  2.  2. -2.  2.  2.  2.  2.  2. -2.  2.  2. -2.\n",
      " -2. -2.]\n",
      "F: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n",
      "Mult: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-127-80d4a857bf41>:58: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.sum( residuos/(np.abs(residuos)*(2 - np.abs(residuos))) )\n",
      "/usr/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-1565cf2666dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mytr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-80d4a857bf41>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \"\"\"\n\u001b[1;32m     94\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mmult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaso_newton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mcheck_X_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             X, y = self._validate_data(X, y,\n\u001b[0m\u001b[1;32m    157\u001b[0m                                        validate_separately=(check_X_params,\n\u001b[1;32m    158\u001b[0m                                                             check_y_params))\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    646\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     96\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "gb = GBCasero(loss=LogLoss(), reg = False)\n",
    "\n",
    "ytr = np.ones_like(ytrain)\n",
    "ytr[ytrain==0] = -1\n",
    "gb.fit(Xtrain, ytr)\n",
    "\n",
    "z = gb.predict(ytr)\n",
    "print(z)\n",
    "\n",
    "plotModel(x1,x2,ytrain,gb)\n",
    "\n",
    "#TODO: medir tiempos, comparar con otro conjunto \"real\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "bibliography.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
