{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Luis Antonio Ortega Andrés    \n",
    "Antonio Coín Castro*\n",
    "# Métodos Avanzados en Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T15:36:50.819579Z",
     "start_time": "2020-11-12T15:36:50.805327Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T15:33:19.827630Z",
     "start_time": "2020-11-12T15:33:19.777532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plotModel(x,y,clase,clf,title=\"\"):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    x_min, x_max = x.min() - .2, x.max() + .2\n",
    "    y_min, y_max = y.min() - .2, y.max() + .2\n",
    "    hx = (x_max - x_min)/100.\n",
    "    hy = (y_max - y_min)/100.\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
    "\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    elif hasattr(clf, \"predict_proba\"):\n",
    "        z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    else:\n",
    "        z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    z = z.reshape(xx.shape)\n",
    "    print(z)\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    plt.contourf(xx, yy, z, cmap=cm, alpha=.8)\n",
    "    plt.contour(xx, yy, z, [0.5], linewidths=[2], colors=['k'])\n",
    "\n",
    "    if clase is not None:\n",
    "        plt.scatter(x[clase==0], y[clase==0], c='#FF0000')\n",
    "        plt.scatter(x[clase==1], y[clase==1], c='#0000FF')\n",
    "    else:\n",
    "        plt.plot(x,y,'g', linewidth=3)\n",
    "        \n",
    "    plt.gca().set_xlim(xx.min(), xx.max())\n",
    "    plt.gca().set_ylim(yy.min(), yy.max())\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T15:33:22.700631Z",
     "start_time": "2020-11-12T15:33:22.657613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def createDataSet(n,model,ymargin,noise=None,output_boundary=False):\n",
    "    x = np.random.rand(n,1)*2.0*np.pi\n",
    "    xbnd = np.linspace(0,2.0*np.pi,100)\n",
    "\n",
    "    if model == 'sine':\n",
    "        y = (np.random.rand(n,1) - 0.5)*2.2\n",
    "        c = y > np.sin(x)\n",
    "        ybnd = np.sin(xbnd)\n",
    "    elif model == 'linear':\n",
    "        y = np.random.rand(n,1)*2.0*np.pi\n",
    "        c = y > x\n",
    "        ybnd = xbnd\n",
    "    elif model == 'square':\n",
    "        y = np.random.rand(n,1)*4.0*np.pi*np.pi\n",
    "        c = y > x*x\n",
    "        ybnd = xbnd*xbnd\n",
    "    else:\n",
    "        y = np.random.rand(n,1)*2.0*np.pi\n",
    "        c = y > x\n",
    "        ybnd = xbnd\n",
    "    \n",
    "    y[c == True] = y[c == True] + ymargin\n",
    "    y[c == False] = y[c == False] - ymargin\n",
    "    \n",
    "    if noise is not None:\n",
    "        y = y + noise * np.random.randn(n,1)\n",
    "        x = x + noise * np.random.randn(n,1)\n",
    "\n",
    "    if output_boundary == True:\n",
    "        return x, y, (c*1).ravel(), xbnd, ybnd\n",
    "    else:\n",
    "        return x, y, (c*1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T15:33:23.248924Z",
     "start_time": "2020-11-12T15:33:23.233703Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plotData(x,y,c,style0,style1,title=''):\n",
    "    plt.scatter(x[c==0],y[c==0],**style0)\n",
    "    plt.scatter(x[c==1],y[c==1],**style1)\n",
    "    plt.grid(True)\n",
    "    plt.axis([x.min()-0.2, x.max()+0.2, y.min()-0.2, y.max()+0.2])\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "El algoritmo de Gradient Boosting persigue construir un clasificador *ensemble* mediante un modelo aditivo, es decir, como suma de clasificadores débiles (*weak classifiers*). La idea general es obtener un clasificador $F(x)$ de la forma\n",
    "\n",
    "$$F(x)=\\sum_{i=1}^M h_m(x),$$\n",
    "\n",
    "de manera que realizamos una construcción iterativa en M pasos:\n",
    "\n",
    "$$F_m(x)=F_{m-1}(x) + \\gamma_mh_m(x), \\quad m=1,\\dots,M.$$\n",
    "\n",
    "Los multiplicadores $\\gamma_m$ son constantes que marcan el tamaño del paso y ponderan el peso de cada clasificador débil. El objetivo es optimizar los valores de $\\gamma_m$ y $h_m$ para minimizar la pérdida entre las predicciones y los valores reales en el conjunto de entrenamiento, utilizando un esquema de gradiente descendente. De esta forma, definimos nuestro clasificador inicial como aquella constante que minimiza la función de pérdida.\n",
    "\n",
    "Pasamos ahora a describir el algoritmo de Gradient Boosting:\n",
    "\n",
    "**Entrada**: \n",
    "\n",
    "- Conjunto de entrenamiento $\\{(x_n, y_n)\\}_{n=1,\\dots,N}$.\n",
    "- Función de pérdida puntual diferenciable $L(y,F(x))$.\n",
    "- Número de número de iteraciones $M$.\n",
    "\n",
    "**Procedimiento**:\n",
    "\n",
    "1. Se inicializa el modelo con un clasificador constante que minimice la pérdida esperada:\n",
    "$$\n",
    "F_0 (x) = \\argmin_{\\gamma \\in \\mathbb R} \\sum_{n=1}^N L(y_n, \\gamma).\n",
    "$$\n",
    "2. Para $m = 1,\\dots,M$:\n",
    "    1. Se calculan los *pseudo-residuos*:\n",
    "    $$\n",
    "        r_{nm} = - \\left[ \\frac{\\partial L(y_n, F(x_n))}{\\partial F(x_n)} \\right]_{F(x) = F_{m-1}(x)} \\quad n = 1,\\dots,N.\n",
    "    $$\n",
    "    \n",
    "    2. Se ajusta un modelo base $h_m(x)$ a los pseudo-residuos, es decir, se entrena $h_m$ sobre el conjunto $\\{x_n, r_{nm}\\}_{n=1,\\dots,N}$.\n",
    "    3. Se estima el multiplicador $\\gamma_m$ que minimiza el error en entrenamiento, utilizando si es necesario un único paso del método de Newton-Raphson:\n",
    "    $$\n",
    "    \\gamma_m = \\argmin_\\gamma \\sum_{n=1}^N L(y_n, F_{m-1}(x_n) + \\gamma h_m(x_n)).\n",
    "    $$\n",
    "    4. Se actualiza el modelo:\n",
    "    $$\n",
    "    F_m(x) =F_{m-1}(x) + \\gamma_m h_m(x).\n",
    "    $$\n",
    "    \n",
    "3. Devolvemos $F_M(x)$ como modelo final en el caso de regresión, o alguna transformación de la misma para clasificación, que transforme valores continuos en etiquetas de clase teniendo en cuenta cuál es la función de pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error cuadrático\n",
    "\n",
    "Para el caso de regresión utilizamos como función de pérdida el error cuadrático entre predicciones y valores reales (convenientemente dividido entre 2 para simplificar los cálculos):\n",
    "\n",
    "$$\n",
    "L(y, F(x)) = \\frac{1}{2}(y - F(x))^2.\n",
    "$$\n",
    "\n",
    "Veamos ahora con detalle cuáles son los elementos que modelan el algoritmo en este caso.\n",
    "\n",
    "1. **Estimador inicial**. El valor que minimiza la pérdida cuadrática coincide con la media de las respuestas de los datos de entrenamiento:\n",
    "$$\n",
    "F_0(x) = \\argmin_\\gamma \\sum_{n=1}^N L(y_n, \\gamma) = \\argmin_\\gamma \\sum_{n=1}^N (y_n - \\gamma)^2\n",
    "$$\n",
    "$$\n",
    "\\dfrac{\\partial \\sum_{n=1}^N L(y_n, \\gamma)}{\\partial \\gamma} = -\\sum_{n=1}^N(y_n - \\gamma) = 0 \\implies F_0(x) = \\frac{1}{N}\\sum_{n=1}^N y_n = \\bar{y}.\n",
    "$$\n",
    "\n",
    "2. **Pseudo-residuos**. Como la función de pérdida es derivable, podemos calcular explícitamente su derivada parcial respecto de la segunda variable:\n",
    "$$\n",
    "r_{nm} = - \\left[ \\frac{\\partial L(y_n, F(x_n))}{\\partial F(x_n)} \\right]_{F(x) = F_{m-1}(x)} = -\\left[ \\frac{\\partial}{\\partial F(x_n)}\\frac{1}{2}(y_n - F(x_n)^2 \\right]_{F(x) = F_{m-1}(x)} = y_n - F_{m-1}(x_n).$$\n",
    "\n",
    "3. **Multiplicador**. Queremos minimizar en $\\gamma$ la siguiente función:\n",
    "$$\n",
    "f_m(\\gamma) = \\sum_{n=1}^N \\frac{1}{2}(y_n - F_{m-1}(x_n) - \\gamma h_m(x_n))^2\n",
    "$$\n",
    "Para ello, pretendemos calcular su derivada e igualarla a $0$. En primer lugar, se tiene:\n",
    "$$\n",
    "f_m'(\\gamma) = \\sum_{n=1}^N -h_m(x_n)(y_n - F_{m-1}(x_n) - \\gamma h_m(x_n)) \n",
    "$$\n",
    "Ahora, como $h_m$ se ha entrenado para que\n",
    "$$\n",
    "h_m(x_n) \\approx r_{nm} = y_n- F_{m-1}(x_n),\n",
    "$$\n",
    "podemos aproximar:\n",
    "$$\n",
    "f_m'(\\gamma) = \\sum_{n=1}^N - (y_n- F_{m-1}(x_n))(y_n- F_{m-1}(x_n) - \\gamma (y_n- F_{m-1}(x_n))) = (\\gamma - 1) \\sum_{n=1}^N (y_n- F_{m-1}(x_n))^2.\n",
    "$$\n",
    "Por tanto, concluimos que:\n",
    "$$\n",
    "f_m'(\\gamma)= 0 \\implies \\gamma_m^* = 1.$$\n",
    "\n",
    "Teniendo todo esto en cuenta, creamos una clase que represente estos elementos para la pérdida cuadrática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T18:44:07.779501Z",
     "start_time": "2020-11-15T18:44:07.697868Z"
    }
   },
   "outputs": [],
   "source": [
    "class SquaredErrorLoss:\n",
    "    \"\"\" \n",
    "    Define elementos para la función de pérdida cuadrática.\n",
    "    \"\"\"\n",
    "    \n",
    "    def F0(_, X, y):\n",
    "        \"\"\" Calcula el valor constate que minimiza la pérdida con la salida 'y'. \n",
    "            Para la pérdida cuadrática este valor coincide con la media de los \n",
    "            valores de 'y'. \"\"\"\n",
    "        \n",
    "        return np.mean(y)\n",
    "\n",
    "    def residuos(_, y, F):\n",
    "        \"\"\" Calcula los pseudo-residuos para un objetivo 'y' y \n",
    "            una salida del modelo 'F'. \"\"\"\n",
    "\n",
    "        return y - F\n",
    "\n",
    "    def paso_newton(_, y, residuos, hm):\n",
    "        \"\"\" Recibe el valor a predecir ('y'), los pseudo-residuos ('residuos') \n",
    "            sobre los que se entrena el nuevo regresor, y la salida \n",
    "            del mismo ('hm'). Calcula el multiplicador asociado al paso m. \"\"\"\n",
    "        \n",
    "        return 1.0\n",
    "    \n",
    "    def __call__(self, y, F):\n",
    "        \"\"\" Devuelve el valor puntual de la función de pérdida para un \n",
    "            objetivo 'y' y una salida del modelo F. \"\"\"\n",
    "        \n",
    "        return (y - F) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error logístico\n",
    "\n",
    "Hacemos el mismo análisis para el caso de clasificación, utilizando como función de pérdida el error logístico:\n",
    "\n",
    "$$\n",
    "L(y, F(x)) = \\log(1 + e^{-2yF(x)}), \\quad y \\in \\{-1,1\\}.\n",
    "$$\n",
    "\n",
    "En este caso la estrategia será un poco distinta, pues aprovecharemos que los estimadores débiles que usamos son árboles de decisión de profundidad $J$, es decir, de la forma \n",
    "\n",
    "$$h_m(x)=\\sum_{j=1}^{J} b_{jm}\\mathcal{I}(x \\in R_{jm}),$$\n",
    "\n",
    "donde $\\mathcal I(\\cdot)$ es la función indicadora y $\\{R_{jm}\\}_{j=1,\\dots,J}$ son las regiones en las que se divide el espacio de entrada, representadas por las hojas terminales del árbol en el paso $m$-ésimo. En este caso, podemos mejorar la calidad del ajuste calculando un multiplicador diferente en cada nodo, es decir, construyendo los estimadores como\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\sum_{j=1}^{J} \\gamma_{jm}\\mathcal{I}(x \\in R_{jm}).$$\n",
    "\n",
    "Notamos que, esto es equivalente a \"descartar\" los valores $\\{b_{jm}\\}$ del árbol de decisión y considerar que los multiplicadores son $\\gamma_{jm}=\\rho_m b_{jm}$, donde $\\rho_m$ serían los multiplicadores a nivel de árbol que usaríamos siguiendo el algoritmo general. Por último, teniendo en cuenta que las regiones $\\{R_{jm}\\}$ son disjuntas, concluimos que\n",
    "\n",
    "$$\\gamma_{jm}= \\argmin_\\gamma \\sum_{x_n \\in R_{jm}} L(y_n, F_{m-1}(x_n)+\\gamma). $$\n",
    "\n",
    "Veamos entonces cómo serían los pasos del algoritmo:\n",
    "\n",
    "1. **Estimador inicial**. En este caso, <a href=\"#Análisis-matemático-de-la-pérdida-logarítmica\">se puede demostrar</a> que el valor del estimador constante inicial es:\n",
    "$$\n",
    "F_0(x)=\\argmin_\\gamma \\sum_{n=1}^N L(y_n, \\gamma) = \\argmin_\\gamma \\sum_{n=1}^N \\log(1 + e^{-2y_nF(x_n)}) = \\frac{1}{2}\\log \\left(\\frac{1+\\bar{y}}{1-\\bar y}\\right).\n",
    "$$\n",
    "\n",
    "2. **Pseudo-residuos**. Se tiene que:\n",
    "$$\n",
    "r_{nm} = - \\left[ \\frac{\\partial L(y_n, F(x_n))}{\\partial F(x_n)} \\right]_{F(x) = F_{m-1}(x)} = -\\left[ \\frac{\\partial \\log(1 + e^{-2y_nF(x_n)})}{\\partial F(x_n)} \\right]_{F(x) = F_{m-1}(x)} = \\frac{2y_n e^{-2y_nF_{m-1}(x_n)}}{1 + e^{-2y_nF_{m-1}(x_n)}} = \\frac{2y_n}{1 + e^{2y_nF_{m-1}(x_n)}}, \\quad n = 1,\\dots, N.\n",
    "$$\n",
    "\n",
    "3. **Multiplicador**. Queremos minimizar en $\\gamma$ las siguientes funciones:\n",
    "$$\n",
    "f_j(\\gamma) = \\sum_{x_n \\in R_{jm}}L(y_n, F_{m-1}(x_n) + \\gamma) = \\sum_{x_n \\in R_{jm}}\\log\\left(1 + e^{-2y_n(F_{m-1}(x_n) + \\gamma)}\\right), \\quad j = 1,\\dots, J.$$\n",
    "Como este problema de optimización es complicado, aproximamos el valor del mínimo estimando la raíz de su derivada mediante un único paso del método de Newton-Rhapson, con condición inicial $\\gamma=0$:\n",
    "$$\n",
    "f_{jm}'(\\gamma) = \\sum_{x_n \\in R_{jm}} \\frac{-2y_n  e^{-2y_n(F_{m-1}(x_n)+\\gamma)}}{1 + e^{-2y_n(F_{m-1}(x_n)+\\gamma)}} = \\sum_{x_n \\in R_{jm}} \\frac{-2y_n}{1 + e^{2y_n(F_{m-1}(x_n)+\\gamma)}},\n",
    "$$\n",
    "$$\n",
    "f_{jm}''(\\gamma) = \\sum_{x_n \\in R_{jm}} \\frac{4y_n^2e^{2y_n(F_{m-1}(x_n)+\\gamma)}}{\\left(1 + e^{2y_n(F_{m-1}(x_n)+\\gamma)}\\right)^2}.\n",
    "$$\n",
    "Por tanto, se tiene que:\n",
    "$$\n",
    "\\gamma_{jm}^{*} = -\\frac{f_{jm}'(\\gamma = 0)}{f_{jm}''(\\gamma = 0)} = \\dfrac{\\displaystyle \\sum_{x_n \\in R_{jm}}r_{nm}}{\\displaystyle \\sum_{x_n \\in R_{jm}} \\dfrac{2y_ne^{2y_nF_{m-1}(x_n)}2y_n}{(1 + e^{2y_nF_{m-1}(x_n)})(1+e^{2y_nF_{m-1}(x_n)})}}.\n",
    "$$\n",
    "Ahora, de la fórmula de $r_{nm}$ deducimos que $e^{2y_nF_{m-1}(x_n)}=\\frac{-2y_n-r_{nm}}{r_{nm}}$, y teniendo en cuenta que $y_n\\in \\{-1, 1\\}$, llegamos a que\n",
    "$$\n",
    "\\gamma_{jm}^{*} =\\dfrac{\\displaystyle \\sum_{x_n \\in R_{jm}} r_{nm}}{\\displaystyle \\sum_{x_n \\in R_{jm}}|r_{nm}|(2-|r_{nm}|)}.\n",
    "$$\n",
    "\n",
    "4. **Transformación de la salida a etiquetas.** Como esta pérdida la usamos para resolver un problema de clasificación, debemos transformar la salida de nuestro regresor $F_M$ en etiquetas de clase. Para ello utilizamos la función *sigmoide*, adaptada convenientemente para el caso en que las etiquetas son $\\pm 1$:\n",
    "$$\n",
    "\\hat \\sigma(x)=\\frac{1}{1 + e^{-2x}}\n",
    "$$\n",
    "De nuevo, <a href=\"#Análisis-matemático-de-la-pérdida-logarítmica\">se puede demostrar</a> que la transformación \n",
    "$$\n",
    "F_M(x) \\mapsto \\hat \\sigma(F_M(x))\n",
    "$$\n",
    "transforma la salida en una probabilidad entre $0$ y $1$, de forma que nuestro modelo final queda como\n",
    "$$\n",
    "P(y=1\\mid x)=\\hat \\sigma(F_M(x)).\n",
    "$$\n",
    "De esta forma, para cada nuevo ejemplo $x$ daremos como predicción la clase $\\hat y(x)\\in\\{-1, 1\\}$ que verifique \n",
    "$$P(y=\\hat y\\mid x)\\ge P(y\\neq \\hat y\\mid x).$$ Dada la simetría entre las etiquetas, y en vista de que la función sigmoide toma el valor $0.5$ en $x=0$, tenemos finalmente que esto equivale a\n",
    "$$\n",
    "\\hat y(x)=\\operatorname{signo}(\\hat \\sigma(F_M(x))).\n",
    "$$\n",
    "\n",
    "*Nota*: no aplicamos esta estrategia en el caso de la pérdida cuadrática porque no proporciona un nuevo algoritmo; el clasificador final seguiría siendo el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:02:23.509588Z",
     "start_time": "2020-11-15T19:02:23.472375Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogLoss:\n",
    "    \"\"\" \n",
    "    Define elementos para la función de pérdida logarítmica.\n",
    "    \"\"\"\n",
    "    \n",
    "    def F0(_, X, y):\n",
    "        \"\"\" Calcula el valor constate que minimiza la pérdida con la salida 'y'. \"\"\"\n",
    "        \n",
    "        ybar = np.mean(y)\n",
    "        \n",
    "        return 0.5 * np.log((1.0 + ybar) / (1.0 - ybar))\n",
    "\n",
    "    def residuos(_, y, F):\n",
    "        \"\"\" Calcula los pseudo-residuos para un objetivo 'y' y \n",
    "            una salida del modelo 'F'. \"\"\"\n",
    "\n",
    "        return 2.0 * y / (1 + np.exp(2.0 * y * F))\n",
    "\n",
    "    def paso_newton(_, y, residuos, hm):\n",
    "        \"\"\" Recibe el valor a predecir ('y'), los pseudo-residuos ('residuos') \n",
    "            sobre los que se entrena el nuevo regresor, y la salida \n",
    "            del mismo ('hm'). Calcula el multiplicador asociado al paso m. \"\"\"\n",
    "        \n",
    "        residuos_abs = np.abs(residuos)\n",
    "        \n",
    "        # TODO: hacer la suma solo en los x_i \\in R_jm\n",
    "        return np.sum(residuos) / np.sum(residuos_abs * (2.0 - residuos_abs))\n",
    "    \n",
    "    def __call__(self, y, F):\n",
    "        \"\"\" Devuelve el valor puntual de la función de pérdida para un \n",
    "            objetivo 'y' y una salida del modelo F. \"\"\"\n",
    "        \n",
    "        return np.log(1 + np.exp(-2.0 * y * F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T19:02:43.323945Z",
     "start_time": "2020-11-15T19:02:43.252160Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: separar en clase abstracta y dos clases que heredan/implementan\n",
    "class GBCasero:\n",
    "    def __init__(self, n_estimators, loss, learning_rate, \n",
    "                 max_depth, subsample, random_state):\n",
    "        self.n_estimators = n_estimators\n",
    "        self._estimators  = []\n",
    "        self._multipliers = []\n",
    "        self.random_state = random_state\n",
    "        self.max_depth        = max_depth\n",
    "        self.learning_rate          = learning_rate\n",
    "        self.subsample  = subsample\n",
    "        self.loss         = loss\n",
    "        self.reg          = reg\n",
    "        \n",
    "    def _transform_output(self, y):\n",
    "        y =  (1/(1 + np.exp(-2*y)) > 0.5).astype(int)\n",
    "        y[ y == 0] = -1\n",
    "        return y\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "          Inicializa GB \n",
    "        \"\"\"\n",
    "        self.F0 = self.loss.F0(X, y)\n",
    "        F = self.F0*np.ones(len(y))\n",
    "        print(\"F:\", F)\n",
    "\n",
    "    \n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth = self.depth, random_state=self.random_state)\n",
    "            \"\"\"\n",
    "              Rellenar esta parte para implementar GB        \n",
    "            \"\"\"\n",
    "            r = self.loss.residuos(y, F)\n",
    "            tree.fit(X,r)\n",
    "            pred = tree.predict(X)\n",
    "            mult = self.loss.paso_newton(y, r, pred)\n",
    "            self._multipliers.append(mult)\n",
    "            self._estimators.append(tree)\n",
    "            \n",
    "            F += mult*pred\n",
    "            print(\"Residuos: \", r)\n",
    "            print(\"F:\", F)\n",
    "            print(\"Mult:\", mult)\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def predict(self,X):\n",
    "        \"\"\" Combinamos los valores de pesos y hs para obtener la 'salida', y:\n",
    "               * Para regresión se devuelve 'salida'\n",
    "               * Para clasificación de 2 clases se calcula la probabilidad\n",
    "                  con una sigmoidal (1 / (1 + np.exp(-2*salida))) y se \n",
    "                  devuelve la clase más probable \"\"\"\n",
    "        salida = self.F0\n",
    "        for i in range(self.n_estimators):\n",
    "            salida += self._multipliers[i]*self._estimators[i].predict(X)\n",
    "                \n",
    "        if self.reg == True:\n",
    "            return salida\n",
    "        \n",
    "        return self._transform_output(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T15:34:09.078916Z",
     "start_time": "2020-11-12T15:34:07.294093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Problem data:\n",
    "np.random.seed(11)\n",
    "import matplotlib.pyplot as plt\n",
    "n = 20\n",
    "model = 'sine'\n",
    "ymargin = 0.\n",
    "noise = 0.0             # <========= Modifica este valor 0 ó 0.3, (antes responde a las cuestiones de arriba)\n",
    "x1, x2, ytrain, xbnd, ybnd = createDataSet(n, model, ymargin, noise, True)\n",
    "x1test, x2test, ytest = createDataSet(n*10, model, ymargin, noise)\n",
    "plotData(x1,x2,ytrain,{'c':'#FF0000'},{'c':'#0000FF'})\n",
    "Xtrain = np.concatenate((x1, x2), axis = 1)\n",
    "Xtest = np.concatenate((x1test, x2test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T18:14:09.142997Z",
     "start_time": "2020-11-14T18:14:09.053106Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -8.881784197001252e-16\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -2.220446049250313e-15\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -1.2878587085651816e-14\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -6.439293542825908e-14\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -3.2374103398069565e-13\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -1.6178169914837781e-12\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -8.08819677899919e-12\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -4.044187207341565e-11\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -2.0220936036707826e-10\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -1.0110463577461815e-09\n",
      "Residuos:  [-1.1  0.9 -1.1  0.9 -1.1 -1.1  0.9  0.9 -1.1  0.9  0.9  0.9  0.9  0.9\n",
      " -1.1  0.9  0.9 -1.1 -1.1 -1.1]\n",
      "F: [0.10033535 0.10033534 0.10033535 0.10033534 0.10033535 0.10033535\n",
      " 0.10033534 0.10033534 0.10033535 0.10033534 0.10033534 0.10033534\n",
      " 0.10033534 0.10033534 0.10033535 0.10033534 0.10033534 0.10033535\n",
      " 0.10033535 0.10033535]\n",
      "Mult: -5.055235341444586e-09\n",
      "Residuos:  [-1.10000001  0.90000001 -1.10000001  0.90000001 -1.10000001 -1.10000001\n",
      "  0.90000001  0.90000001 -1.10000001  0.90000001  0.90000001  0.90000001\n",
      "  0.90000001  0.90000001 -1.10000001  0.90000001  0.90000001 -1.10000001\n",
      " -1.10000001 -1.10000001]\n",
      "F: [0.10033538 0.10033532 0.10033538 0.10033532 0.10033538 0.10033538\n",
      " 0.10033532 0.10033532 0.10033538 0.10033532 0.10033532 0.10033532\n",
      " 0.10033532 0.10033532 0.10033538 0.10033532 0.10033532 0.10033538\n",
      " 0.10033538 0.10033538]\n",
      "Mult: -2.527617493086609e-08\n",
      "Residuos:  [-1.10000003  0.90000003 -1.10000003  0.90000003 -1.10000003 -1.10000003\n",
      "  0.90000003  0.90000003 -1.10000003  0.90000003  0.90000003  0.90000003\n",
      "  0.90000003  0.90000003 -1.10000003  0.90000003  0.90000003 -1.10000003\n",
      " -1.10000003 -1.10000003]\n",
      "F: [0.10033552 0.10033521 0.10033552 0.10033521 0.10033552 0.10033552\n",
      " 0.10033521 0.10033521 0.10033552 0.10033521 0.10033521 0.10033521\n",
      " 0.10033521 0.10033521 0.10033552 0.10033521 0.10033521 0.10033552\n",
      " 0.10033552 0.10033552]\n",
      "Mult: -1.2638087865113334e-07\n",
      "Residuos:  [-1.10000017  0.90000014 -1.10000017  0.90000014 -1.10000017 -1.10000017\n",
      "  0.90000014  0.90000014 -1.10000017  0.90000014  0.90000014  0.90000014\n",
      "  0.90000014  0.90000014 -1.10000017  0.90000014  0.90000014 -1.10000017\n",
      " -1.10000017 -1.10000017]\n",
      "F: [0.10033622 0.10033464 0.10033622 0.10033464 0.10033622 0.10033622\n",
      " 0.10033464 0.10033464 0.10033622 0.10033464 0.10033464 0.10033464\n",
      " 0.10033464 0.10033464 0.10033622 0.10033464 0.10033464 0.10033622\n",
      " 0.10033622 0.10033622]\n",
      "Mult: -6.319045540159607e-07\n",
      "Residuos:  [-1.10000086  0.9000007  -1.10000086  0.9000007  -1.10000086 -1.10000086\n",
      "  0.9000007   0.9000007  -1.10000086  0.9000007   0.9000007   0.9000007\n",
      "  0.9000007   0.9000007  -1.10000086  0.9000007   0.9000007  -1.10000086\n",
      " -1.10000086 -1.10000086]\n",
      "F: [0.10033969 0.10033179 0.10033969 0.10033179 0.10033969 0.10033969\n",
      " 0.10033179 0.10033179 0.10033969 0.10033179 0.10033179 0.10033179\n",
      " 0.10033179 0.10033179 0.10033969 0.10033179 0.10033179 0.10033969\n",
      " 0.10033969 0.10033969]\n",
      "Mult: -3.159526774876298e-06\n",
      "Residuos:  [-1.1000043   0.90000352 -1.1000043   0.90000352 -1.1000043  -1.1000043\n",
      "  0.90000352  0.90000352 -1.1000043   0.90000352  0.90000352  0.90000352\n",
      "  0.90000352  0.90000352 -1.1000043   0.90000352  0.90000352 -1.1000043\n",
      " -1.1000043  -1.1000043 ]\n",
      "F: [0.10035707 0.10031758 0.10035707 0.10031758 0.10035707 0.10035707\n",
      " 0.10031758 0.10031758 0.10035707 0.10031758 0.10031758 0.10031758\n",
      " 0.10031758 0.10031758 0.10035707 0.10031758 0.10031758 0.10035707\n",
      " 0.10035707 0.10035707]\n",
      "Mult: -1.579773389304151e-05\n",
      "Residuos:  [-1.1000215   0.90001759 -1.1000215   0.90001759 -1.1000215  -1.1000215\n",
      "  0.90001759  0.90001759 -1.1000215   0.90001759  0.90001759  0.90001759\n",
      "  0.90001759  0.90001759 -1.1000215   0.90001759  0.90001759 -1.1000215\n",
      " -1.1000215  -1.1000215 ]\n",
      "F: [0.10044396 0.10024648 0.10044396 0.10024648 0.10044396 0.10044396\n",
      " 0.10024648 0.10024648 0.10044396 0.10024648 0.10024648 0.10024648\n",
      " 0.10024648 0.10024648 0.10044396 0.10024648 0.10024648 0.10044396\n",
      " 0.10044396 0.10044396]\n",
      "Mult: -7.89911701937207e-05\n",
      "Residuos:  [-1.10010753  0.90008798 -1.10010753  0.90008798 -1.10010753 -1.10010753\n",
      "  0.90008798  0.90008798 -1.10010753  0.90008798  0.90008798  0.90008798\n",
      "  0.90008798  0.90008798 -1.10010753  0.90008798  0.90008798 -1.10010753\n",
      " -1.10010753 -1.10010753]\n",
      "F: [0.10087852 0.09989093 0.10087852 0.09989093 0.10087852 0.10087852\n",
      " 0.09989093 0.09989093 0.10087852 0.09989093 0.09989093 0.09989093\n",
      " 0.09989093 0.09989093 0.10087852 0.09989093 0.09989093 0.10087852\n",
      " 0.10087852 0.10087852]\n",
      "Mult: -0.0003950183779846128\n",
      "Residuos:  [-1.10053772  0.90043999 -1.10053772  0.90043999 -1.10053772 -1.10053772\n",
      "  0.90043999  0.90043999 -1.10053772  0.90043999  0.90043999  0.90043999\n",
      "  0.90043999  0.90043999 -1.10053772  0.90043999  0.90043999 -1.10053772\n",
      " -1.10053772 -1.10053772]\n",
      "F: [0.10305391 0.09811107 0.10305391 0.09811107 0.10305391 0.10305391\n",
      " 0.09811107 0.09811107 0.10305391 0.09811107 0.09811107 0.09811107\n",
      " 0.09811107 0.09811107 0.10305391 0.09811107 0.09811107 0.10305391\n",
      " 0.10305391 0.10305391]\n",
      "Mult: -0.0019766561784559933\n",
      "Residuos:  [-1.10269064  0.90220252 -1.10269064  0.90220252 -1.10269064 -1.10269064\n",
      "  0.90220252  0.90220252 -1.10269064  0.90220252  0.90220252  0.90220252\n",
      "  0.90220252  0.90220252 -1.10269064  0.90220252  0.90220252 -1.10269064\n",
      " -1.10269064 -1.10269064]\n",
      "F: [0.11399539 0.08915894 0.11399539 0.08915894 0.11399539 0.11399539\n",
      " 0.08915894 0.08915894 0.11399539 0.08915894 0.08915894 0.08915894\n",
      " 0.08915894 0.08915894 0.11399539 0.08915894 0.08915894 0.11399539\n",
      " 0.11399539 0.11399539]\n",
      "Mult: -0.00992252767119961\n",
      "Residuos:  [-1.11350415  0.91107656 -1.11350415  0.91107656 -1.11350415 -1.11350415\n",
      "  0.91107656  0.91107656 -1.11350415  0.91107656  0.91107656  0.91107656\n",
      "  0.91107656  0.91107656 -1.11350415  0.91107656  0.91107656 -1.11350415\n",
      " -1.11350415 -1.11350415]\n",
      "F: [0.17035153 0.04304797 0.17035153 0.04304797 0.17035153 0.17035153\n",
      " 0.04304797 0.04304797 0.17035153 0.04304797 0.04304797 0.04304797\n",
      " 0.04304797 0.04304797 0.17035153 0.04304797 0.04304797 0.17035153\n",
      " 0.17035153 0.17035153]\n",
      "Mult: -0.050611525146961434\n",
      "Residuos:  [-1.16872259  0.9569786  -1.16872259  0.9569786  -1.16872259 -1.16872259\n",
      "  0.9569786   0.9569786  -1.16872259  0.9569786   0.9569786   0.9569786\n",
      "  0.9569786   0.9569786  -1.16872259  0.9569786   0.9569786  -1.16872259\n",
      " -1.16872259 -1.16872259]\n",
      "F: [ 0.49809234 -0.22531421  0.49809234 -0.22531421  0.49809234  0.49809234\n",
      " -0.22531421 -0.22531421  0.49809234 -0.22531421 -0.22531421 -0.22531421\n",
      " -0.22531421 -0.22531421  0.49809234 -0.22531421 -0.22531421  0.49809234\n",
      "  0.49809234  0.49809234]\n",
      "Mult: -0.28042651779865535\n",
      "Residuos:  [-1.46061556  1.22157727 -1.46061556  1.22157727 -1.46061556 -1.46061556\n",
      "  1.22157727  1.22157727 -1.46061556  1.22157727  1.22157727  1.22157727\n",
      "  1.22157727  1.22157727 -1.46061556  1.22157727  1.22157727 -1.46061556\n",
      " -1.46061556 -1.46061556]\n",
      "F: [ 4.2293045  -3.34589167  4.2293045  -3.34589167  4.2293045   4.2293045\n",
      " -3.34589167 -3.34589167  4.2293045  -3.34589167 -3.34589167 -3.34589167\n",
      " -3.34589167 -3.34589167  4.2293045  -3.34589167 -3.34589167  4.2293045\n",
      "  4.2293045   4.2293045 ]\n",
      "Mult: -2.5545477327505313\n",
      "Residuos:  [-1.99957596  1.99752094 -1.99957596  1.99752094 -1.99957596 -1.99957596\n",
      "  1.99752094  1.99752094 -1.99957596  1.99752094  1.99752094  1.99752094\n",
      "  1.99752094  1.99752094 -1.99957596  1.99752094  1.99752094 -1.99957596\n",
      " -1.99957596 -1.99957596]\n",
      "F: [ 33571.22146023 -33535.84040243  33571.22146023 -33535.84040243\n",
      "  33571.22146023  33571.22146023 -33535.84040243 -33535.84040243\n",
      "  33571.22146023 -33535.84040243 -33535.84040243 -33535.84040243\n",
      " -33535.84040243 -33535.84040243  33571.22146023 -33535.84040243\n",
      " -33535.84040243  33571.22146023  33571.22146023  33571.22146023]\n",
      "Mult: -16787.055301027103\n",
      "Residuos:  [-2.  2. -2.  2. -2. -2.  2.  2. -2.  2.  2.  2.  2.  2. -2.  2.  2. -2.\n",
      " -2. -2.]\n",
      "F: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n",
      "Mult: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-0cb41415c2f9>:18: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.sum( residuos/(np.abs(residuos)*(2 - np.abs(residuos))) )\n",
      "/home/antcc/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:90: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3a3ba8fa74db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mytr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3c4e87f724ad>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \"\"\"\n\u001b[1;32m     30\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mmult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaso_newton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mcheck_X_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             X, y = self._validate_data(X, y,\n\u001b[0m\u001b[1;32m    157\u001b[0m                                        validate_separately=(check_X_params,\n\u001b[1;32m    158\u001b[0m                                                             check_y_params))\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "gb = GBCasero(loss=LogLoss(), reg = False)\n",
    "\n",
    "ytr = np.ones_like(ytrain)\n",
    "ytr[ytrain==0] = -1\n",
    "gb.fit(Xtrain, ytr)\n",
    "\n",
    "z = gb.predict(ytr)\n",
    "#print(z)\n",
    "\n",
    "plotModel(x1,x2,ytrain,gb)\n",
    "\n",
    "#TODO: medir tiempos, comparar con otro conjunto \"real\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "\n",
    "- Plot con los clasificadores en cada etapa (mirar en los notebooks de teoría)\n",
    "- Añadir regularización: shrinkage, subsampling y limit tree depth. Enlazar artículos; ESLR\n",
    "- Utilizar en conjunto real; hacer CV con M + regularización y encontrar los parámetros óptimos.\n",
    "- Plot train/test error vs number of iterations on a single dataset.\n",
    "- Comparar con sklearn\n",
    "- Median loss?\n",
    "- Separar en baseGB, classGB y regGB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis matemático de la pérdida logarítmica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Por qué usamos la pérdida log para clasificación (adaptada a $\\pm 1$)? (binary cross-entropy, max log-likelihood)\n",
    "- ¿Por qué usamos sigmoide? Se elige primero sigmoide y luego logloss o al revés?\n",
    "- Deducción de $F_0$\n",
    "- Por qué se puede usar $\\sigma(F(x))$ para recuperar la probabilidad de $y=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "bibliography.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
